"""
Generate S1 values and violation percentage for agriculture-related tickers.

Reads yfinance_tickers.csv, downloads returns, computes Bell-CHSH S1 per pair
in rolling 20-day windows, and outputs Results/s1_values.csv and Results/violation_pct.csv.
"""

import numpy as np
import pandas as pd
import yfinance as yf
import itertools
from tqdm import tqdm
from datetime import date
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.express as px
import os
import plotly.io as pio

pio.renderers.default = "browser"

#========================================================================================================
#Data
#========================================================================================================

def yf_stocks():
    """Load tickers from yfinance_tickers.csv (generated by scripts/build_ticker_universe.py)."""
    df = pd.read_csv("yfinance_tickers.csv")
    tickers = df["ticker"].tolist()
    return tickers

def download_yf_data(tickers, START_DATE = "2000-01-01", END_DATE   = date.today().strftime("%Y-%m-%d")):
    '''Returns percent change in adjusted close price for a list of Yahoo Finance tickers between a specified START_DATE and END_DATE'''
    print(f"Downloading data for {len(tickers)} tickers from {START_DATE} to {END_DATE}...")
    raw = yf.download(tickers, start=START_DATE, end=END_DATE,
                    group_by="ticker", auto_adjust=True, threads=True, progress=False)

    adj_close = pd.DataFrame()
    for t in tickers:
        try:
            adj_close[t] = raw[t]['Close']
        except Exception:
            continue

    adj_close = adj_close.dropna(axis=1, how="all")
    returns = adj_close.pct_change().dropna(how="all").sort_index()
    print(f"Final usable tickers: {len(returns.columns)}")
    print(f"Returns shape: {returns.shape}")
    return returns

#========================================================================================================
#Computation
#========================================================================================================

def compute_s1_sliding_pair(x, y, window_size=20, q=0.95, std_mult=2.0, fixed_thr=0.05, mode="fixed", return_min_cell=False):

    #Creates windows
    n = x.shape[0]
    m = n - window_size
    if m <= 0:
        return (np.array([]), np.array([])) if return_min_cell else np.array([])

    shape = (m, window_size)
    stride = x.strides[0]
    x_win = np.lib.stride_tricks.as_strided(x, shape=shape, strides=(stride, stride)).copy()
    y_win = np.lib.stride_tricks.as_strided(y, shape=shape, strides=(stride, stride)).copy()

    #Calculates sign and magnitude for each element in the window
    a_sgn, b_sgn = np.sign(x_win), np.sign(y_win)
    abs_x, abs_y = np.abs(x_win), np.abs(y_win)

    #Applies a mask for a fixed threshold
    if mode == "fixed":
        thr_x = fixed_thr
        thr_y = fixed_thr
        mask_x0 = abs_x >= thr_x
        mask_y0 = abs_y >= thr_y

    #Applies a mask for a quantile based threshold
    elif mode == "percentile":
        thr_x = np.quantile(abs_x, q, axis=1)
        thr_y = np.quantile(abs_y, q, axis=1)
        mask_x0 = abs_x >= thr_x[:, None]
        mask_y0 = abs_y >= thr_y[:, None]

    #Applies a mask for a standard deviation based threshold
    elif mode == "std":
        thr_x = abs_x.std(axis=1) * std_mult
        thr_y = abs_y.std(axis=1) * std_mult
        mask_x0 = abs_x >= thr_x[:, None]
        mask_y0 = abs_y >= thr_y[:, None]

    else:
        raise ValueError("mode must be one of {'fixed', 'percentile', 'std'}")

    # Cell counts for each of the 4 CHSH settings (sparse cells → unreliable S1)
    cnt_ab = (mask_x0 & mask_y0).sum(axis=1)
    cnt_abp = (mask_x0 & ~mask_y0).sum(axis=1)
    cnt_apb = (~mask_x0 & mask_y0).sum(axis=1)
    cnt_apbp = (~mask_x0 & ~mask_y0).sum(axis=1)
    min_cell = np.minimum(np.minimum(cnt_ab, cnt_abp), np.minimum(cnt_apb, cnt_apbp))

    #Calculates expectations as the average sign product of terms in the mask
    def E(mask):
        term = (a_sgn * b_sgn) * mask
        s = term.sum(axis=1)
        cnt = mask.sum(axis=1)
        e = np.zeros_like(s, dtype=float)
        nz = cnt > 0
        e[nz] = s[nz] / cnt[nz]
        return e

    s1 = E(mask_x0 & ~mask_y0) + E(mask_x0 & mask_y0) + E(~mask_x0 & mask_y0) - E(~mask_x0 & ~mask_y0)
    return (s1, min_cell) if return_min_cell else s1

def run_computation(
    returns,
    OUTPUT_PCT_CSV="violation_pct.csv",
    OUTPUT_S1_CSV="s1_values.csv",
    OUTPUT_S1_SUPPLEMENT="s1_values_supplement.csv",
    WINDOW_SIZE=20,
    mode="fixed",
    fixed_thr=0.05,
    THRESHOLD_Q=0.95,
    STD_MULT=2.0,
    BOUND=2,
    MIN_CELL_COUNT=3,
    USE_FILTERED=True,
):
    '''Creates S1 values and violation percentage. Main outputs use filtered (MinCellCount >= MIN_CELL_COUNT) when USE_FILTERED=True.

    USE_FILTERED: If True (default), s1_values.csv and ViolationPct contain only reliable pairs. Supplement has unfiltered.
    MIN_CELL_COUNT: minimum observations per CHSH cell for "reliable" (default 3).
    '''
    dates = returns.index[WINDOW_SIZE:]
    violation_counts = np.zeros(len(dates), dtype=int)
    total_counts = np.zeros(len(dates), dtype=int)
    violation_counts_reliable = np.zeros(len(dates), dtype=int)
    total_counts_reliable = np.zeros(len(dates), dtype=int)

    s1_records = []

    pairs = itertools.combinations(returns.columns, 2)
    for A, B in tqdm(pairs, total=(len(returns.columns)*(len(returns.columns)-1))//2,
                     desc="Pairs"):
        ts = returns[[A, B]].dropna()
        if ts.shape[0] <= WINDOW_SIZE:
            continue

        x, y = ts[A].values, ts[B].values
        S1, min_cell = compute_s1_sliding_pair(
            x, y,
            window_size=WINDOW_SIZE,
            q=THRESHOLD_Q,
            std_mult=STD_MULT,
            fixed_thr=fixed_thr,
            mode=mode,
            return_min_cell=True,
        )

        if S1.size == 0:
            continue

        pair_dates = ts.index[WINDOW_SIZE:]
        pos = np.searchsorted(dates, pair_dates)
        valid = (pos >= 0) & (pos < len(dates))
        pos, S1, pair_dates, min_cell = pos[valid], S1[valid], pair_dates[valid], min_cell[valid]

        reliable = min_cell >= MIN_CELL_COUNT

        np.add.at(total_counts, pos, 1)
        np.add.at(violation_counts, pos, (np.abs(S1) > BOUND).astype(int))
        np.add.at(total_counts_reliable, pos, reliable.astype(int))
        np.add.at(violation_counts_reliable, pos, (reliable & (np.abs(S1) > BOUND)).astype(int))

        for d, s, mc in zip(pair_dates, S1, min_cell):
            s1_records.append((d, A, B, s, int(mc)))

    # Main vs supplement: USE_FILTERED=True → main=filtered, supplement=all
    if USE_FILTERED:
        pct_main = np.full(len(dates), np.nan, dtype=float)
        v = total_counts_reliable > 0
        pct_main[v] = 100 * violation_counts_reliable[v] / total_counts_reliable[v]
        pct_supp = np.where(total_counts > 0, 100 * violation_counts / total_counts, np.nan)
    else:
        pct_main = np.where(total_counts > 0, 100 * violation_counts / total_counts, np.nan)
        pct_supp = np.full(len(dates), np.nan, dtype=float)
        v = total_counts_reliable > 0
        pct_supp[v] = 100 * violation_counts_reliable[v] / total_counts_reliable[v]

    out_cols = {
        "Date": dates,
        "ViolationPct": pct_main,
        "TotalPairs": total_counts_reliable if USE_FILTERED else total_counts,
        "ViolationCounts": violation_counts_reliable if USE_FILTERED else violation_counts,
        "ViolationPct_supplement": pct_supp,
        "TotalPairs_supplement": total_counts if USE_FILTERED else total_counts_reliable,
        "ViolationCounts_supplement": violation_counts if USE_FILTERED else violation_counts_reliable,
    }
    out_df = pd.DataFrame(out_cols)
    out_df.to_csv(OUTPUT_PCT_CSV, index=False)
    print(f"Saved percent violation to {OUTPUT_PCT_CSV} (main={'filtered' if USE_FILTERED else 'all'})")

    if s1_records:
        s1_df = pd.DataFrame(s1_records, columns=["Date", "PairA", "PairB", "S1", "MinCellCount"])
        s1_df = s1_df.sort_values("Date")
        reliable_df = s1_df[s1_df["MinCellCount"] >= MIN_CELL_COUNT]
        if USE_FILTERED:
            reliable_df.to_csv(OUTPUT_S1_CSV, index=False)
            s1_df.to_csv(OUTPUT_S1_SUPPLEMENT, index=False)
            print(f"Saved {OUTPUT_S1_CSV} (filtered, MinCellCount>={MIN_CELL_COUNT})")
            print(f"Saved {OUTPUT_S1_SUPPLEMENT} (supplement, unfiltered)")
        else:
            s1_df.to_csv(OUTPUT_S1_CSV, index=False)
            reliable_df.to_csv(OUTPUT_S1_SUPPLEMENT, index=False)
            print(f"Saved {OUTPUT_S1_CSV} (unfiltered)")
            print(f"Saved {OUTPUT_S1_SUPPLEMENT} (supplement, filtered)")
    else:
        print("No S1 data to save.")

    return out_df

#========================================================================================================
#Composite Functions (Final)
#========================================================================================================

def yf_stock_data_plotly():
    os.makedirs("Results", exist_ok=True)
    tickers = yf_stocks()
    returns = download_yf_data(tickers, "2000-01-01", date.today().strftime("%Y-%m-%d"))
    returns.to_csv("Results/returns.csv")
    print("Saved Results/returns.csv")
    use_filtered = os.environ.get("USE_FILTERED", "1").lower() in ("1", "true", "yes")
    run_computation(
        returns,
        OUTPUT_PCT_CSV="Results/violation_pct.csv",
        OUTPUT_S1_CSV="Results/s1_values.csv",
        OUTPUT_S1_SUPPLEMENT="Results/s1_values_supplement.csv",
        WINDOW_SIZE=20,
        mode="fixed",
        BOUND=2,
        USE_FILTERED=use_filtered,
    )

yf_stock_data_plotly()