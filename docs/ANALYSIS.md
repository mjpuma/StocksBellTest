# Methodology: Bell Inequality Violations in Agriculture Stock Networks

This document provides a complete methodology section suitable for an academic paper, including all mathematical definitions, data specifications, and procedural details.

---

## 1. Data and Stock Selection

### 1.1 Stock Universe

We use an **agriculture-focused stock universe** defined by the constituents of the VanEck Agribusiness ETF (MOO). Zarifian et al. (2025) analyzed the S&P 500; we focus on the agriculture sector to differentiate our work and target firms with direct exposure to agricultural commodity markets.

**Selection methodology:** The universe is built from (1) MOO US-listed constituents (curated list from VanEck holdings) and (2) ag-adjacent tickers from the same sector (fertilizers, seeds, equipment, food processing, farmland REITs). The list is generated by `scripts/build_ticker_universe.py`, which fetches sector and industry from Yahoo Finance (GICS-based) and maps each ticker to an objective group via `config/industry_to_group.csv` and `config/ticker_to_group_override.csv`.

**Groups:** Fertilizers, Seeds & Crop Protection, Farm Machinery & Equipment, Animal Health, Agricultural Trading & Processing, Food Processing, Food Distribution, Farmland REIT, Aquaculture, Retail, Other.

**Category (MNC vs Pure Ag):** Each ticker is classified as **MNC** (multinational) if it is listed on a non-US exchange (e.g., BAYN.DE, YAR.OL), has market cap ≥ $50B, or is explicitly designated in `config/ticker_to_category_override.csv`. Otherwise it is **Pure Ag**. This allows stratified analysis and coloring in network visualizations.

**Scope options:** `--scope moo` (US only, ~15 tickers), `--scope moo_plus` (~37), `--scope global` (~43, includes international: Bayer, Yara, Mowi, Kubota, SalMar, K+S).

**Stratified stats:** `scripts/compute_category_stats.py` outputs `Results/category_stats.csv` and `Results/group_stats.csv` with mean degree, clustering, and betweenness by Date × Category and Date × Group for stratified analysis.

**Filters:** No market-cap floor by default; optional `--min-cap` excludes micro-caps. Tickers with missing or delisted data are dropped at download time.

**Date range:** 2000-01-01 through the most recent trading date at run time.

**Ticker list (sample):** The full list is in `yfinance_tickers.csv` with columns `ticker`, `name`, `sector`, `industry`, `group`, `category`, `market_cap_approx`. Representative entries:

| Ticker | Name | Sector | Group |
|--------|------|--------|-------|
| DE | Deere & Company | Industrials | Farm Machinery & Equipment |
| CTVA | Corteva, Inc. | Basic Materials | Seeds & Crop Protection |
| NTR | Nutrien Ltd. | Basic Materials | Fertilizers |
| ADM | Archer-Daniels-Midland | Consumer Defensive | Agricultural Trading & Processing |
| TSN | Tyson Foods | Consumer Defensive | Agricultural Trading & Processing |
| ZTS | Zoetis Inc. | Healthcare | Animal Health |
| LAND | Gladstone Land Corporation | Real Estate | Farmland REIT |

### 1.2 Returns

For each ticker, we compute daily **percent change in adjusted close price** (log returns are not used):

$$r_{i,t} = \frac{P_{i,t} - P_{i,t-1}}{P_{i,t-1}}$$

where \(P_{i,t}\) is the adjusted close on date \(t\). Rows with missing returns are dropped.

### 1.3 Commodity Volatility Index

We use the **S&P GSCI** (ticker ^SPGSCI) as the commodity volatility benchmark. The S&P GSCI is a world-production-weighted index of 24 physically delivered commodity futures, including Brent Crude, WTI Crude, Heating Oil, RBOB Gasoline, Gasoil, Natural Gas, Aluminum, Copper, Nickel, Lead, Zinc, Gold, Silver, Corn, Soybeans, Chicago Wheat, Kansas Wheat, Cotton, Sugar, Coffee, Cocoa, Live Cattle, Feeder Cattle, and Lean Hogs. WTI is weighted at approximately 25%, Brent at 18%, Corn at 5%, and Live Cattle at 4%.

**Date range for S&P GSCI:** 2000-01-01 to 2025-10-14 (hardcoded in 1.py).

---

## 2. Bell-CHSH S₁ and Violation Percentage

### 2.1 Sliding-Window Construction

For each unordered pair of tickers \((A, B)\), we form a bivariate return series and apply a **rolling window of 20 trading days**. Let \(x_t\) and \(y_t\) denote the returns of tickers A and B on day \(t\). For each window \(w\) ending on date \(t\), we have vectors \(\mathbf{x}_w = (x_{t-19}, \ldots, x_t)\) and \(\mathbf{y}_w = (y_{t-19}, \ldots, y_t)\).

### 2.2 Sign and Threshold Masks

Define the **sign** of each return as \(a_i = \mathrm{sign}(x_i)\) and \(b_i = \mathrm{sign}(y_i)\). We use a **fixed threshold** \(\tau = 0.05\) (5%) to define binary outcomes:

- **Above threshold:** \(|\cdot| \geq 0.05\)
- **Below threshold:** \(|\cdot| < 0.05\)

Four masks are defined:

- \(m_{ab}\): both \(|x_i| \geq \tau\) and \(|y_i| \geq \tau\)
- \(m_{ab'}\): \(|x_i| \geq \tau\) and \(|y_i| < \tau\)
- \(m_{a'b}\): \(|x_i| < \tau\) and \(|y_i| \geq \tau\)
- \(m_{a'b'}\): both \(|x_i| < \tau\) and \(|y_i| < \tau\)

### 2.3 Expectation Terms

For each mask \(m\), the expectation is the **average sign product** over days where the mask is true:

$$\mathbb{E}(a,b) = \frac{\sum_{i \in w} a_i \, b_i \, m_i}{\sum_{i \in w} m_i}$$

with the convention that the sum is zero (and the expectation undefined) when the denominator is zero. Here \(a_i b_i \in \{-1, +1\}\).

### 2.4 Bell-CHSH S₁

The CHSH-type correlator is:

$$S_1 = \mathbb{E}(a,b) + \mathbb{E}(a,b') + \mathbb{E}(a',b) - \mathbb{E}(a',b')$$

Under any **local hidden-variable (LHV) theory**, the Bell-CHSH inequality implies \(|S_1| \leq 2\). Values \(|S_1| > 2\) therefore indicate **Bell inequality violations** that cannot be explained by LHV models.

### 2.5 Violation Percentage

Let \(N^{\text{viol}}_t\) denote the number of ticker pairs \((A,B)\) with valid data and \(|S_1^{(A,B)}(t)| > 2\) on date \(t\), and let \(\text{TotalPairs}_t\) be the total number of pairs with valid \(S_1\) on that date. We define:

$$\text{ViolationPct}_t = 100 \times \frac{N^{\text{viol}}_t}{\text{TotalPairs}_t}$$

This yields one time series of violation percentage per day.

### 2.6 Rationale and Justification

#### Why S₁ Violation Is Not "Just Correlation"

A natural concern is that observed violations might be explained by ordinary correlation or by a common hidden factor (e.g., market-wide shocks). This concern is addressed by the structure of the Bell-CHSH inequality.

**Ordinary correlation** (e.g., Pearson or Spearman) between two variables is bounded in \([-1, 1]\). Such correlations can always be reproduced by a **local hidden-variable model**: a common factor \(\lambda\) (e.g., "market sentiment") that influences both outcomes, with no need for nonlocal or nonclassical effects. In that setting, the joint distribution factors as \(P(a,b|\lambda) = P(a|\lambda)P(b|\lambda)\), and any correlator built from such a model satisfies classical bounds.

The **CHSH quantity** \(S_1\) is a specific linear combination of four correlators, each conditioned on different measurement settings (here: different threshold masks). Bell (1964) and Clauser, Horne, Shimony, and Holt (1969) proved that under **any** local hidden-variable theory—i.e., any model in which outcomes depend only on local information and a shared hidden variable \(\lambda\)—the inequality \(|S_1| \leq 2\) must hold. The bound 2 is **derived**, not chosen: it follows from the algebra of LHV models.

Therefore, if we observe \(|S_1| > 2\), we can conclude that **no** local hidden-variable model can reproduce the data. The correlation structure is incompatible with any explanation that relies solely on a common factor and local responses. This is the core of Bell's theorem: the bound 2 is a necessary consequence of locality and realism in the sense of pre-existing values.

#### Why the Threshold 2

The value 2 is the **Bell bound** (the maximum attainable under LHV theories). It is not a tuning parameter. For the CHSH form used here:

- **Classical (LHV):** \(|S_1| \leq 2\)
- **Quantum:** \(|S_1| \leq 2\sqrt{2} \approx 2.83\)
- **General no-signaling:** \(|S_1| \leq 4\)

We classify \(|S_1| > 2\) as a violation because that is the threshold at which LHV explanations are ruled out. Values between 2 and \(2\sqrt{2}\) are compatible with quantum-like correlations but not with classical hidden variables.

#### Why the Return Threshold \(\tau = 0.05\)

The CHSH test requires **binary outcomes** (e.g., "up" vs "down" or "above threshold" vs "below threshold") for each measurement. We obtain these by thresholding the magnitude of returns at \(\tau = 0.05\) (5%).

**Justification for 5%:**

1. **Economic interpretation:** A 5% daily move is widely treated as a "significant" or "large" move in stock markets. It corresponds roughly to a two- to three-standard-deviation event in typical daily volatility. Using this threshold separates "normal" fluctuations from "event-like" moves.

2. **Stability of the Bell test:** The CHSH inequality is defined for binary outcomes. If the threshold is too low, almost all days are "above," and the test degenerates. If it is too high, almost all days are "below," and we lose discriminatory power. A 5% threshold yields a non-trivial split in both marginals across typical market conditions.

3. **Robustness:** The codebase supports alternative modes (percentile-based or standard-deviation-based thresholds) for sensitivity analysis. The fixed 5% threshold provides a transparent, reproducible baseline. Prior work in finance has used similar cutoffs (e.g., 5% for "large" daily moves) for consistency with the literature.

#### Addressing the "Hidden Variable" Critique

Reviewers may ask: "Could a hidden variable explain the correlations?"

The answer is: **not if \(|S_1| > 2\).** By definition, any model that attributes the outcomes to a shared hidden variable \(\lambda\) and local response functions satisfies the LHV assumptions. Bell's theorem shows that all such models obey \(|S_1| \leq 2\). Therefore:

- If \(|S_1| \leq 2\): the data are compatible with (but do not prove) an LHV explanation.
- If \(|S_1| > 2\): **no** LHV model can fit the data. The violation is a mathematical consequence of the CHSH inequality.

The threshold 2 is thus the dividing line between "explainable by hidden variables" and "not explainable by hidden variables." It is not a matter of convention or choice; it is the boundary implied by Bell's theorem.

### 2.7 Cell-Count Reliability and Filtering

The four CHSH expectation terms are computed over subsets of the 20-day window. When a cell (e.g., both |r| ≥ 5%) has few observations, the E estimate is unstable and |S₁| can spuriously exceed 2 or even 2.83. To address this:

- **MinCellCount:** For each (Date, PairA, PairB), we record the minimum observation count across the four cells. Low values (0–2) indicate unreliable S₁.
- **Filtering (default):** With `USE_FILTERED=True`, `s1_values.csv` and ViolationPct contain only pair-dates with MinCellCount ≥ 3. Unfiltered data is in `s1_values_supplement.csv` and ViolationPct_supplement. Set `USE_FILTERED=False` in `0.py` to swap main and supplement.
- **Usage:** Downstream scripts read `s1_values.csv` and thus use filtered data by default.

### 2.8 Outputs

- **s1_values.csv:** Date, PairA, PairB, S1, MinCellCount (filtered when USE_FILTERED=True)
- **s1_values_supplement.csv:** Unfiltered S₁ (or filtered when USE_FILTERED=False)
- **violation_pct.csv:** Date, ViolationPct, TotalPairs, ViolationCounts, ViolationPct_supplement, TotalPairs_supplement, ViolationCounts_supplement

---

## 3. Volatility Measures

### 3.1 Rolling Realized Volatility

$$\sigma_t^{\text{roll}} = \sqrt{252} \times \text{std}(r_{t-19}, \ldots, r_t)$$

with a 20-day window, annualized by \(\sqrt{252}\).

### 3.2 GARCH(1,1)

The conditional variance follows:

$$\sigma_t^2 = \omega + \alpha_1 \epsilon_{t-1}^2 + \beta_1 \sigma_{t-1}^2$$

Fitted via the `arch` package. The annualized volatility is \(\sqrt{252} \times \sigma_t\).

### 3.3 Regime-Switching Volatility

A two-regime Markov regression with switching variance is fitted to S&P GSCI returns. The **regime-switching volatility** is:

$$\sigma_t^{\text{regime}} = p_t \times \sqrt{252} \times \text{std}(r_{t-19}, \ldots, r_t)$$

where \(p_t\) is the smoothed probability of the high-variance regime. This series is used for event identification.

---

## 4. Event Identification

### 4.1 Extreme and Normal Periods

We classify each trading day as **extreme** or **normal** using the regime-switching volatility:

- **Extreme:** \(\sigma_t^{\text{regime}} > 0.40\) (40% annualized)
- **Normal:** \(\sigma_t^{\text{regime}} \leq 0.40\)

### 4.2 Block Detection

Consecutive days with the same classification form **blocks**. An extreme block is a maximal run of days with \(\sigma_t^{\text{regime}} > 0.40\). A normal block is a maximal run with \(\sigma_t^{\text{regime}} \leq 0.40\).

### 4.3 Event Selection

We retain only extreme blocks that are **immediately preceded** by a normal block (no gap). Among these, we select the **top 3** by:

1. Block length (number of trading days), descending
2. Mean regime volatility within the block, descending

These correspond to:

- **Event 1:** 2008 Financial Crisis
- **Event 2:** COVID-19
- **Event 3:** Ukraine War

Each event is a pair (extreme block, preceding normal block).

---

## 5. Network Construction

### 5.1 Daily Networks

For each trading date, we build an **undirected graph** \(G_t\):

- **Nodes:** All tickers that appear in the S1 dataset (typically 42 for the agriculture universe)
- **Edges:** An edge between tickers A and B exists if and only if \(|S_1^{(A,B)}(t)| > 2\) (Bell inequality violation; correlations incompatible with classical hidden-variable models)

Edge weights are the corresponding \(S_1\) values. Isolated nodes are retained.

### 5.2 Global Network Metrics

| Metric | Symbol | Definition |
|--------|--------|------------|
| **Density** | \(d\) | \(d = \frac{2m}{n(n-1)}\) where \(m\) = edges, \(n\) = nodes |
| **Giant component size** | \(\mathcal{G}\) | \(\mathcal{G} = \frac{\max_i |C_i|}{|N|}\) (fraction of nodes in largest connected component) |
| **Average clustering** | \(C\) | \(C = \frac{1}{n} \sum_{v \in G} c_v\), \(c_v\) = local clustering of node \(v\) |
| **Global clustering** | — | Transitivity: fraction of possible triangles that exist |
| **Efficiency** | \(E\) | \(E(G) = \frac{1}{n(n-1)} \sum_{i \neq j} \frac{1}{d(i,j)}\) over largest component |
| **Average shortest path** | \(L\) | \(L(G) = \frac{1}{n(n-1)} \sum_{i \neq j} d(i,j)\) over largest component |
| **Diameter** | — | \(\max_{i,j} d(i,j)\) over largest component |
| **Degree centralization** | \(C_D\) | \(C_D = \frac{\sum_i (k_{\max} - k_i)}{(n-1)(n-2)}\) |
| **Betweenness centralization** | \(C_B\) | \(C_B = \frac{\sum_i (b_{\max} - b_i)}{(n-1)(n-2)}\) |
| **Modularity** | \(Q\) | \(Q = \frac{1}{2m} \sum_{i,j} \left[A_{ij} - \frac{k_i k_j}{2m}\right] \delta(c_i, c_j)\) (greedy modularity) |
| **Community size entropy** | \(H\) | \(H = -\sum_i p_i \log p_i\), \(p_i\) = fraction of nodes in community \(i\) |
| **Number of communities** | — | Count from greedy modularity |
| **Assortativity** | \(r\) | Pearson correlation of degrees of connected nodes |
| **Scale-free exponent** | \(\alpha\) | \(P(k) \propto k^{-\alpha}\) fitted by method of moments |

### 5.3 Node-Level Metrics

- **Degree** \(k_i = \sum_j A_{ij}\)
- **Clustering coefficient** \(c_i = \frac{2e_i}{k_i(k_i-1)}\) where \(e_i\) = edges among neighbors of \(i\)
- **Betweenness centrality** \(b_i = \sum_{s \neq i \neq t} \frac{\sigma_{st}(i)}{\sigma_{st}}\)
- **Closeness centrality** \(c_i^{\text{close}} = \frac{n-1}{\sum_j d(i,j)}\)
- **Eigenvector centrality** (largest eigenvector of adjacency matrix)

---

## 6. Bootstrap Confidence Intervals

For each event and each metric, we compute **95% bootstrap confidence intervals**:

1. Extract all non-NaN values of the metric over the period (extreme or normal).
2. Draw \(B = 5000\) bootstrap samples (resample with replacement, same size).
3. Compute the mean of each bootstrap sample.
4. Set:
   - \(\text{lo} = \text{percentile}(\text{boot\_means}, 2.5)\)
   - \(\text{med} = \text{percentile}(\text{boot\_means}, 50)\)
   - \(\text{hi} = \text{percentile}(\text{boot\_means}, 97.5)\)

Random seed: 12345.

---

## 7. Permutation Tests

### 7.1 Null Hypothesis

For each metric and each event (or aggregate), the null is: **extreme and normal samples come from the same distribution**.

### 7.2 Test Statistic

$$\Delta_{\text{obs}} = \bar{x} - \bar{y}$$

where \(\bar{x}\) = mean in extreme period, \(\bar{y}\) = mean in normal period.

### 7.3 Permutation Procedure

1. Pool all observations: \(n = n_x + n_y\).
2. For \(k = 1, \ldots, N_{\text{perm}}\) (with \(N_{\text{perm}} = 100{,}000\)):
   - Randomly permute the pooled sample.
   - Split into two groups of sizes \(n_x\) and \(n_y\).
   - Compute \(\Delta^{(k)} = \bar{x}^{(k)} - \bar{y}^{(k)}\).
3. Two-sided p-value:
   $$p = \frac{\#\{k : |\Delta^{(k)}| \geq |\Delta_{\text{obs}}|\} + 1}{N_{\text{perm}} + 1}$$

### 7.4 Effect Size: Cohen's d

$$d = \frac{\bar{x} - \bar{y}}{s_p}$$

where the pooled standard deviation is:

$$s_p = \sqrt{\frac{(n_x - 1) s_x^2 + (n_y - 1) s_y^2}{n_x + n_y - 2}}$$

### 7.5 Multiple Testing

Benjamini–Hochberg FDR correction is applied across metrics:

$$q_i = \min_{j \geq i} \frac{m}{j} p_{(j)}$$

where \(p_{(1)} \leq \cdots \leq p_{(m)}\). Metrics with \(p < 0.05\) or \(q < 0.05\) are flagged as significant.

---

## 8. Granger Causality

We test **Granger causality** between violation percentage and regime-switching volatility in both directions.

### 8.1 Data

- **Violation %:** Daily time series from `violation_pct.csv` (one value per date).
- **Regime volatility:** Daily time series from `regime_vol.csv`.

Both series are merged on date (inner join) and any missing values are dropped.

### 8.2 Test

For each direction (Violation → Volatility and Volatility → Violation) and for lags \(\ell = 1, \ldots, 20\), we run a Granger causality test using the F-test from the restricted vs. unrestricted regression (SSR-based). The `statsmodels` function `grangercausalitytests` is used with `verbose=False`.

### 8.3 Output

Results are saved with Direction, Lag, p-value, and q-value (here q-value equals p-value; no FDR correction across lags/directions in the current implementation).

---

## 9. Lead–Lag Timing

For each crisis window (2008, COVID-19, Ukraine), we identify the date of peak violation % and the date of peak regime volatility within the window. The lead–lag \(\Delta\) (days) is violation peak minus volatility peak; positive \(\Delta\) means violation peaks after volatility.

**Stock-universe sensitivity:** Lead–lag patterns depend on the stock universe. The agriculture-focused set (MOO constituents) shows different timing than broader universes (e.g., S&P 500). For example, in 2008 the agriculture set exhibits a violation peak in March 2009 (well after the November 2008 volatility peak), whereas broader universes may show violation and volatility peaks closer in time. This suggests sector-specific dynamics in when Bell violations emerge relative to commodity volatility.

---

## 10. Software and Parameters Summary

| Parameter | Value |
|-----------|-------|
| Rolling window (S1, volatility) | 20 trading days |
| Fixed threshold (S1) | 0.05 |
| Violation bound | \|S1\| > 2 |
| Edge threshold (network) | S1 > 2 |
| Volatility extreme threshold | 0.40 (40% annualized) |
| Bootstrap iterations | 5000 |
| Permutation iterations | 100,000 |
| Granger max lag | 20 |
| Random seed (bootstrap) | 12345 |
